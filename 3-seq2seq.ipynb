{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation with Torchtext!! ## \n",
    "\n",
    "Seq2Seq network with torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer(tokenizer=None) #'spacy', language='de')\n",
    "en_tokenizer = get_tokenizer(tokenizer=None) #'spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    src_max_length = 0\n",
    "    tgt_max_length = 0\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "        \n",
    "        if de_tensor_.size(0) > src_max_length:\n",
    "            src_max_length = de_tensor_.size(0)\n",
    "            \n",
    "        if en_tensor_.size(0) > tgt_max_length:\n",
    "            tgt_max_length = en_tensor_.size(0)\n",
    "        \n",
    "    return data, src_max_length, tgt_max_length\n",
    "\n",
    "train_data, trsl, trtl = data_process(train_filepaths)\n",
    "val_data, vsl, vtl = data_process(val_filepaths)\n",
    "test_data, tesl, tetl = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# We calculate the sentence with max_length\n",
    "candidates_lengths = [trsl, trtl, vsl, vtl, tesl, tetl]\n",
    "print(max(candidates_lengths))\n",
    "MAX_LENGTH = max(candidates_lengths) + 2\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, en_batch_out = [], [], []\n",
    "    a = 0\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_extra = MAX_LENGTH - (de_item.size(0) + 2)\n",
    "        en_extra = MAX_LENGTH - (en_item.size(0) + 2)\n",
    "        en_inp_extra = MAX_LENGTH - (en_item.size(0) + 1)\n",
    "        \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX]), torch.full((de_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX]), torch.full((en_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch_out.append(torch.cat([en_item, torch.tensor([EOS_IDX]), torch.full((en_inp_extra,), PAD_IDX)], dim=0)) # Target input \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    en_batch_out = pad_sequence(en_batch_out, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch, en_batch_out\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_id_to_text = lambda x: [de_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_to_text = lambda x: [en_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we create the Seq2Seq model ##\n",
    "\n",
    "The Seq2Seq model is an encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, gru_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru_layers = gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, gru_layers)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder ###\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, gru_layers, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.gru_layers = gru_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, gru_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        batch_size = x.size()[0]\n",
    "        # Embedding: a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings\n",
    "#         print(\"INPUT SHAPE: \", x.shape)\n",
    "#         print(batch_size)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  create the classic train/evaluate loop ###\n",
    "\n",
    "For this task, we have something called **teacher_forcing_ratio**, this helps us vary between giving the network the possibility to try to translate using its own previous prediction (no teacher forcing), or we use the known target for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # We run the input sequence through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         print(encoder_output.shape)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.full((target_tensor.size(1),), BOS_IDX, device=device)\n",
    "#     decoder_input = torch.tensor([[BOS_IDX], target_tensor.size(1)], device=device)\n",
    "\n",
    "#     print(decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "            if di + 1 < target_length:\n",
    "                decoder_input = target_tensor[di + 1]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "#             print(target_tensor[di].shape)\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "\n",
    "#     print(decoded_words)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # We run the input sequence through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "#         print(encoder_outputs[:1])\n",
    "        decoder_input = torch.full((input_tensor.size(1),), BOS_IDX, device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "#             print(decoder_input)\n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(english_id_to_text(topi))\n",
    "#             print(decoded_words)\n",
    "            decoder_input = topi.detach()\n",
    "          \n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, epochs, print_every=50, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    PAD_IDX = en_vocab.stoi['<pad>']\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, epochs +1):\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        print(\"-----------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" out of \", epochs, \" epochs\")\n",
    "        print(\"-----------------------\")\n",
    "        for batch_counter, (src, trg, trg_output) in enumerate(train_iter):\n",
    "            src, trg, trg_output = src.to(device), trg.to(device), trg_output.to(device)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "            trg_output_ = trg_output.permute(1, 0)\n",
    "            \n",
    "            # Uncomment to go 1 by 1 manual unbatch version (preferably simply set batch = 1)\n",
    "#             for j in range(len(src_)):\n",
    "#                 loss = train(src_[j].unsqueeze(dim=1), trg_[j].unsqueeze(dim=1), trg_output_[j].unsqueeze(dim=1), encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#                 print_loss_total += loss\n",
    "#                 plot_loss_total += loss\n",
    "                \n",
    "            loss = train(src, trg, trg_output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "    #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                       for i in range(n_iters)]\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, batch_counter / len(train_iter)),\n",
    "                                             batch_counter, batch_counter/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "            if batch_counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,951,616 trainable parameters\n",
      "The model has 9,662,349 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  1  epochs\n",
      "-----------------------\n",
      "0m 11s (50 22%) 5.2251\n",
      "0m 23s (100 44%) 4.2727\n",
      "0m 35s (150 66%) 3.9138\n",
      "0m 47s (200 88%) 3.8743\n",
      "-----------------------\n",
      "EPOCH:  1  out of  1  epochs\n",
      "-----------------------\n",
      "1m 4s (50 22%) 3.7265\n",
      "1m 16s (100 44%) 3.7429\n",
      "1m 27s (150 66%) 3.9775\n",
      "1m 38s (200 88%) 3.6867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(INPUT_DIM, hidden_size, 4).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, OUTPUT_DIM, 4, dropout_p=0.1).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, epochs = 1, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly evaluate\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for _, (src, trg, tgt_output) in enumerate(test_iter):\n",
    "        for i in range(n):\n",
    "#             print(src.shape)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "#             print(src_[i])\n",
    "#             print(src_[i].unsqueeze(dim=1).shape)\n",
    "\n",
    "            src_ = src_[i]\n",
    "            trg_ = trg_[i]\n",
    "            print('>', german_id_to_text(src_))\n",
    "            print('=', english_id_to_text(trg_))\n",
    "            output_words, attentions = evaluate(encoder, decoder, src_.unsqueeze(dim=1))\n",
    "#             output_sentence = ' '.join(output_words)\n",
    "            print('<', output_words)\n",
    "            print('')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['<bos>', 'Ein', 'Mann', 'auf', 'einem', 'Motorrad', 'zeigt', 'einen', 'Trick', 'auf', 'einer', 'Rennstrecke.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "= ['<bos>', 'A', 'man', 'riding', 'a', 'motorcycle', 'is', 'performing', 'a', 'trick', 'at', 'a', 'track.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "< [['A'], ['man'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation is hard task!!\n",
    "For this work: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "Training duration is quite long, as well as the dataset is no sufficient for machine translation network. However, in this case it is enough in order to under stand the purpose and learn the new techniques.\n",
    "\n",
    "Implement and test Pytorch’s own Transformer models for machine translation\n",
    "Analyze based on the output and measurements.\n",
    "Compare to provided example seq2seq\n",
    "\n",
    "# Lab Assignment Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding +\n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of our model and instantiate it.\n",
    "# Define loss function and optimizer - Train model further down.\n",
    "SRC_VOCAB_SIZE = len(de_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 3 #not too many epochs\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"cpu\")\n",
    "#print(\"device\", DEVICE)\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer,print_every=50, plot_every=100,):\n",
    "    start_time = time.time()\n",
    "    plot_losses = []\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, sample in enumerate(train_iter):\n",
    "        #print(\"s\", len(sample))\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "        src = sample[0]\n",
    "        tgt = sample[1]\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if idx % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / BATCH_SIZE\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start_time, (idx+1) / len(train_iter)),\n",
    "                                        idx, (idx+1)/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "        if idx % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / BATCH_SIZE\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    end_time = time.time()\n",
    "    return losses / len(train_iter), plot_losses\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, sample in (enumerate(valid_iter)):\n",
    "       \n",
    "        src = sample[0]\n",
    "        tgt = sample[1]\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                  src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    #print(\"s1\", len(sample))\n",
    "    return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "EPOCH:  1  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0759\n",
      "0m 12s (50 22%) 0.0509\n",
      "0m 24s (100 44%) 0.0430\n",
      "0m 36s (150 66%) 0.0404\n",
      "0m 48s (200 88%) 0.0373\n",
      "Epoch: 1, Train loss: 5.865, Val loss: 4.564\n",
      "-----------------------\n",
      "EPOCH:  2  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0363\n",
      "0m 12s (50 22%) 0.0345\n",
      "0m 24s (100 44%) 0.0334\n",
      "0m 36s (150 66%) 0.0333\n",
      "0m 48s (200 88%) 0.0297\n",
      "Epoch: 2, Train loss: 4.189, Val loss: 3.749\n",
      "-----------------------\n",
      "EPOCH:  3  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0302\n",
      "0m 12s (50 22%) 0.0280\n",
      "0m 24s (100 44%) 0.0267\n",
      "0m 37s (150 66%) 0.0269\n",
      "0m 49s (200 88%) 0.0273\n",
      "Epoch: 3, Train loss: 3.561, Val loss: 3.347\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(\"-----------------------\")\n",
    "    print(\"EPOCH: \", epoch, \" out of \", NUM_EPOCHS, \" epochs\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    train_loss, plot_losses = train_epoch(transformer, train_iter, optimizer)\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    #train_loss = train_epoch(transformer, optimizer)\n",
    "    #val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))\n",
    "\n",
    "#showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(device)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
    "    model.eval()\n",
    "    tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
    "    num_tokens = len(tokens)\n",
    "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A group of people standing in front of a building. '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", de_vocab, en_vocab, de_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Below\n",
    "For this work: https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "### Generating names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('C:/Users/jason/Python/Jason/Done/2-language-models-lab/data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (10000 12%) 2.8473\n",
      "0m 32s (20000 25%) 2.8129\n",
      "0m 48s (30000 37%) 1.9738\n",
      "1m 4s (40000 50%) 2.3870\n",
      "1m 20s (50000 62%) 1.6632\n",
      "1m 36s (60000 75%) 3.0329\n",
      "1m 52s (70000 87%) 1.8133\n",
      "2m 8s (80000 100%) 1.5035\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "n_iters = 80000\n",
    "print_every = 10000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ff1ad68d60>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Loss change during training')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------ENG---------\n",
      "Aler\n",
      "Jaller\n",
      "Marta\n",
      "---------RUS---------\n",
      "Roullov\n",
      "Uantan\n",
      "Santov\n",
      "---------KOR---------\n",
      "Ka\n",
      "Lo\n",
      "Ion\n",
      "---------GRE---------\n",
      "Garisos\n",
      "Tarisos\n",
      "Sallas\n",
      "---------CHI---------\n",
      "Cha\n",
      "Lan\n",
      "Han\n"
     ]
    }
   ],
   "source": [
    "name_length = 20\n",
    "\n",
    "# Sample from a country/language and starting letter\n",
    "def sample(language, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(language)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(name_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get several samples from one country/language\n",
    "# and starting letters of each name\n",
    "def samples(language, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(language, start_letter))\n",
    "        \n",
    "print(\"---------ENG---------\")\n",
    "samples('English','AJM')\n",
    "print(\"---------RUS---------\")\n",
    "samples('Russian', 'RUS')\n",
    "print(\"---------KOR---------\")\n",
    "samples('Korean', 'KLI')\n",
    "print(\"---------GRE---------\")\n",
    "samples('Greek', 'GTS')\n",
    "print(\"---------CHI---------\")\n",
    "samples('Chinese', 'CLH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMLdl] *",
   "language": "python",
   "name": "conda-env-AMLdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
