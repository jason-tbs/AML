{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation with Torchtext!! ## \n",
    "\n",
    "Seq2Seq network with torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer(tokenizer=None) #'spacy', language='de')\n",
    "en_tokenizer = get_tokenizer(tokenizer=None) #'spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    src_max_length = 0\n",
    "    tgt_max_length = 0\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "        \n",
    "        if de_tensor_.size(0) > src_max_length:\n",
    "            src_max_length = de_tensor_.size(0)\n",
    "            \n",
    "        if en_tensor_.size(0) > tgt_max_length:\n",
    "            tgt_max_length = en_tensor_.size(0)\n",
    "        \n",
    "    return data, src_max_length, tgt_max_length\n",
    "\n",
    "train_data, trsl, trtl = data_process(train_filepaths)\n",
    "val_data, vsl, vtl = data_process(val_filepaths)\n",
    "test_data, tesl, tetl = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "# We calculate the sentence with max_length\n",
    "candidates_lengths = [trsl, trtl, vsl, vtl, tesl, tetl]\n",
    "print(max(candidates_lengths))\n",
    "MAX_LENGTH = max(candidates_lengths) + 2\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, en_batch_out = [], [], []\n",
    "    a = 0\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_extra = MAX_LENGTH - (de_item.size(0) + 2)\n",
    "        en_extra = MAX_LENGTH - (en_item.size(0) + 2)\n",
    "        en_inp_extra = MAX_LENGTH - (en_item.size(0) + 1)\n",
    "        \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX]), torch.full((de_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX]), torch.full((en_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch_out.append(torch.cat([en_item, torch.tensor([EOS_IDX]), torch.full((en_inp_extra,), PAD_IDX)], dim=0)) # Target input \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    en_batch_out = pad_sequence(en_batch_out, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch, en_batch_out\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_id_to_text = lambda x: [de_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_to_text = lambda x: [en_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we create the Seq2Seq model ##\n",
    "\n",
    "The Seq2Seq model is an encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, gru_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru_layers = gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, gru_layers)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder ###\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, gru_layers, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.gru_layers = gru_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, gru_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        batch_size = x.size()[0]\n",
    "        # Embedding: a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings\n",
    "#         print(\"INPUT SHAPE: \", x.shape)\n",
    "#         print(batch_size)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  create the classic train/evaluate loop ###\n",
    "\n",
    "For this task, we have something called **teacher_forcing_ratio**, this helps us vary between giving the network the possibility to try to translate using its own previous prediction (no teacher forcing), or we use the known target for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # We run the input sequence through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         print(encoder_output.shape)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.full((target_tensor.size(1),), BOS_IDX, device=device)\n",
    "#     decoder_input = torch.tensor([[BOS_IDX], target_tensor.size(1)], device=device)\n",
    "\n",
    "#     print(decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "            if di + 1 < target_length:\n",
    "                decoder_input = target_tensor[di + 1]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "#             print(target_tensor[di].shape)\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "\n",
    "#     print(decoded_words)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # We run the input sequence through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "#         print(encoder_outputs[:1])\n",
    "        decoder_input = torch.full((input_tensor.size(1),), BOS_IDX, device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "#             print(decoder_input)\n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(english_id_to_text(topi))\n",
    "#             print(decoded_words)\n",
    "            decoder_input = topi.detach()\n",
    "          \n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, epochs, print_every=50, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    PAD_IDX = en_vocab.stoi['<pad>']\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, epochs +1):\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        print(\"-----------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" out of \", epochs, \" epochs\")\n",
    "        print(\"-----------------------\")\n",
    "        for batch_counter, (src, trg, trg_output) in enumerate(train_iter):\n",
    "            src, trg, trg_output = src.to(device), trg.to(device), trg_output.to(device)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "            trg_output_ = trg_output.permute(1, 0)\n",
    "            \n",
    "            # Uncomment to go 1 by 1 manual unbatch version (preferably simply set batch = 1)\n",
    "#             for j in range(len(src_)):\n",
    "#                 loss = train(src_[j].unsqueeze(dim=1), trg_[j].unsqueeze(dim=1), trg_output_[j].unsqueeze(dim=1), encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#                 print_loss_total += loss\n",
    "#                 plot_loss_total += loss\n",
    "                \n",
    "            loss = train(src, trg, trg_output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "    #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                       for i in range(n_iters)]\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, batch_counter / len(train_iter)),\n",
    "                                             batch_counter, batch_counter/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "            if batch_counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,951,616 trainable parameters\n",
      "The model has 9,662,349 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  1  epochs\n",
      "-----------------------\n",
      "0m 12s (50 22%) 5.1915\n",
      "0m 25s (100 44%) 4.2880\n",
      "0m 38s (150 66%) 4.0578\n",
      "0m 52s (200 88%) 3.8964\n",
      "-----------------------\n",
      "EPOCH:  1  out of  1  epochs\n",
      "-----------------------\n",
      "1m 12s (50 22%) 3.7835\n",
      "1m 25s (100 44%) 3.8095\n",
      "1m 38s (150 66%) 3.6956\n",
      "1m 51s (200 88%) 3.7462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfaElEQVR4nO3deXSV9b3v8fc380gCSYCEAGGUAElQaEWpVFGPIkNpLdR127ra23tdtOu6rGhRq9WeclqvONR7Otzq8nh6zvXca4N1IIhaFWernqBJmBEIkwkkIZKEzMPv/pEtQghkQ3Z49t75vNbKMjvPj+Tz+Cw+eXj28/x+5pxDRERCX4TXAUREJDBU6CIiYUKFLiISJlToIiJhQoUuIhImorz6wenp6S4nJ8erHy8iEpI2btxY45zL6G2bZ4Wek5NDcXGxVz9eRCQkmdm+023TJRcRkTChQhcRCRMqdBGRMKFCFxEJEyp0EZEwoUIXEQkTKnQRkTARcoVeWdfML9duob2zy+soIiJBJeQKvfRAHX9+fy+/27DL6ygiIkEl5Ar92ukj+daFo/jDG7soPXDU6zgiIkEj5Aod4L7F0xieHMuthSW0tHd6HUdEJCiEZKGnxEez+tv57Klu5IGXt3sdR0QkKIRkoQNcNimDGy8Zy7++t5f3d9d4HUdExHMhW+gAd86fwrj0RH62poz6lnav44iIeCqkCz0hJoqHlxVQWdfMqqKtXscREfFUSBc6wEVjhvLjyyewZuNBXt162Os4IiKeCflCB7jlysnkZg7hrmfLOHKs1es4IiKeCItCj4mK4JFlBdQ3d3D3c5txznkdSUTkvAuLQgfIzRzCrVdP5uUth3i+5DOv44iInHdhU+gAN80dz8yxQ7n3hS1UHG32Oo6IyHkVVoUeGWE8sqyAzi7HymfK6OrSpRcRGTzCqtABxqYl8vPrcnl3Vw1PfXjaxbFFRMJO2BU6wHcvHsPcyRn8Zv02ymsavY4jInJehGWhmxmrr88nJjKCFYUldGjudBEZBMKy0AFGpsSxasl0Ptl/lMfe3uN1HBGRARe2hQ6wuCCLBXmZPPraTrZU1HkdR0RkQIV1oZsZq5ZMJzUhhhV/KaW1Q3Oni0j4CutCBxiWGMMD1+ex43ADv331U6/jiIgMmLAvdIB5U0Zww1dG89jbuyneW+t1HBGRATEoCh3gnoVTGZUaz21rSmls7fA6johIwA2aQk+KjeKhpQXsr23iN+u3eR1HRCTgBk2hA8wen8aP5ozjPz7cz1s7q72OIyISUH4XuplFmtknZrbuNNsvN7MSM9tiZm8FLmJg3X7NBUwansTKZ0qpa9KydSISPs7mDP0WoNdrFWaWCvwRWOycmwYs7X+0gREXHckjy2Zw5Fgb967d7HUcEZGA8avQzSwbWAA8cZoh/wV41jm3H8A5VxWYeAMjLzuFm+dN4oWSCl4sq/Q6johIQPh7hv4osBI43aQok4GhZvammW00sxt7G2RmN5lZsZkVV1d7ew37J1dMoCA7hXue30RVfYunWUREAqHPQjezhUCVc27jGYZFATPpPou/BviFmU3uOcg597hzbpZzblZGRsa5Zg6I6MgIHl42g6a2Tu58dpOWrRORkOfPGfocYLGZ7QWeBuaZ2VM9xhwEXnbONTrnaoC3gYKAJh0AE4cncce1U9iwvYrC4gNexxER6Zc+C905d5dzLts5lwPcAGxwzn2vx7AXgMvMLMrMEoCLOc0bqMHmB5fmcMn4NH5VtJUDtU1exxEROWfnfB+6mS03s+UAzrltwMtAGfAR8IRzLiRuIYmIMB5cmo+ZcduaUi1bJyIhy7y6djxr1ixXXFzsyc/uTWHxAVY+U8Y9C3L5b5eN9zqOiEivzGyjc25Wb9sG1ZOiZ7J0ZjZX5Y5g9Ss72Hm4wes4IiJnTYXuY2bc/608kmKjWFFYQruWrROREKNCP0FGciy/XjKdzZ/V87sNu7yOIyJyVlToPczPy+SbF47iD2/sovTAUa/jiIj4TYXei18unkZGUiwrCktoadeydSISGlTovUiJj+bBpfnsrm5k9cs7vI4jIuIXFfppXDYpgxsvGcuT75Xz/u4ar+OIiPRJhX4Gd86fwrj0RH62poyGFs2dLiLBTYV+BgkxUTy8rIDKumZ+VbTV6zgiImekQu/DRWOG8uPLJ7Bm40Fe3XrY6zgiIqelQvfDLVdOJjdzCHc9W8aRY61exxER6ZUK3Q8xURE8sqyA+uYO7n5us+ZOF5GgpEL3U27mEG69ejIvbznE8yWfeR1HROQUKvSzcNPc8cwcO5R7X9hCZV2z13FERE6iQj8LkRHGw0sL6Oh0/GxNmeZOF5GgokI/Sznpidy9IJd3d9Xw1If7vI4jInKcCv0cfPfiMcydnMFv1m+jvKbR6zgiIoAK/ZyYGauvzycmMoIVhSV0aO50EQkCKvRzNDIljlVLpvPJ/qM89vYer+OIiKjQ+2NxQRYL8jJ59LWdbK2o9zqOiAxyKvR+MDNWLZlOakIMKwpLaO3Q3Oki4h0Vej8NS4zhgevz2H6ogd+++qnXcURkEFOhB8C8KSO44Sujefzt3RTvrfU6jogMUir0ALln4VSyUuO5bU0pja0dXscRkUFIhR4gSbFRPLS0gP21Tdz/0jav44jIIKRCD6DZ49P40ZxxPPXBft7aWe11HBEZZFToAXb7NRcwaXgSK58ppa5Jy9aJyPmjQg+wuOhIHlk2gyPH2rh37Wav44jIIKJCHwB52SncPG8SL5RU8GJZpddxRGSQUKEPkJ9cMYH87BTueX4TVQ0tXscRkUHA70I3s0gz+8TM1p1hzFfMrNPMvh2YeKErOrJ72bqmtk7u+usmLVsnIgPubM7QbwFOez+emUUCDwCv9DdUuJg4PJk7rp3C69urKCw+4HUcEQlzfhW6mWUDC4AnzjDsZuCvQFUAcoWNH1yawyXj0/hV0VYO1DZ5HUdEwpi/Z+iPAiuBXif+NrNRwDeBP53pm5jZTWZWbGbF1dWD4z7tiAjjwaX5mBm3rSnVsnUiMmD6LHQzWwhUOec2nmHYo8AdzrkzTjfonHvcOTfLOTcrIyPj7JKGsOyhCdy7aCofldfy5HvlXscRkTDlzxn6HGCxme0FngbmmdlTPcbMAp72jfk28EczWxLAnCFv6cxsrsodwepXdvDp4Qav44hIGOqz0J1zdznnsp1zOcANwAbn3Pd6jBnnnMvxjXkG+Ilz7vkByBuyzIz7v5VHUmwUtxaW0K5l60QkwM75PnQzW25mywMZJtxlJMfy6yXT2fxZPb/fsMvrOCISZqLOZrBz7k3gTd/nvb4B6pz7QX9DhbP5eZl888JR/P6NXcybMpyC0aleRxKRMKEnRT3wy8XTyEiKZUVhCS3tWrZORAJDhe6BlPhoHlyaz+7qRla/vMPrOCISJlToHrlsUgY3XjKWJ98r5/3dNV7HEZEwoEL30J3zp5CTlsDP1pTR0KK500Wkf1ToHkqIieLhZTOorGtm1bqtXscRkRCnQvfYzLFDWf71CRQWH+S1rYe9jiMiIUyFHgR+etVkcjOHcOezZRw51up1HBEJUSr0IBAT1T13en1zB/c8v1lzp4vIOVGhB4nczCHcevVkXtp8iBdKKryOIyIhSIUeRG6aO56ZY4fyixc2U1nX7HUcEQkxKvQgEhlhPLy0gI5Ox8pnynTpRUTOigo9yOSkJ3L3glze+bSGpz7Y53UcEQkhKvQg9N2LxzB3cga/Xr+N8ppGr+OISIhQoQchM2P19fnEREZwW2EJHZo7XUT8oEIPUiNT4li1ZDof7z/KY2/v8TqOiIQAFXoQW1yQxYK8TB59bSdbK+q9jiMiQU6FHsTMjFVLppMSH8OKwhJaOzR3uoicngo9yA1LjOGB6/PYfqiBR1/71Os4IhLEVOgh4MrcEXxn1mgee2s3G/fVeh1HRIKUCj1E3LMwl6zUeFYUltLY2uF1HBEJQir0EJEcF81DSwvYX9vE/S9t8zqOiAQhFXoImT0+jR/NGcdTH+znrZ3VXscRkSCjQg8xt19zAZOGJ7HymVLqmrRsnYh8SYUeYuKiI3lk2QyOHGvjvrWbvY4jIkFEhR6C8rJTuHneJJ4vqeDFskqv44hIkFChh6ifXDGB/OwU7nl+E1UNLV7HEZEgoEIPUdGR3cvWNbV1ctdfN2nudBFRoYeyicOTWXntFF7fXkVh8QGv44iIx1ToIe6Hl+Ywe/wwflW0lQO1TV7HEREPqdBDXESE8dDSAsyM29eU0tWlSy8ig5XfhW5mkWb2iZmt62Xbd82szPfxvpkVBDamnEn20ATuXTSVD8trefK9cq/jiIhHzuYM/RbgdM+clwNfd87lA6uAx/sbTM7O0pnZXJU7nNWv7ODTww1exxERD/hV6GaWDSwAnuhtu3Pufefc576XHwDZgYkn/jIz7v9WPkmxUawoLKVdy9aJDDr+nqE/CqwE/GmJHwEv9bbBzG4ys2IzK66u1lwkgZaRHMuvl0xn02d1/H7DLq/jiMh51mehm9lCoMo5t9GPsVfQXeh39LbdOfe4c26Wc25WRkbGWYeVvs3Py+SbF47i92/souzgUa/jiMh55M8Z+hxgsZntBZ4G5pnZUz0HmVk+3ZdkvuGcOxLQlHJWfrl4GhlJsdz6lxJa2rVsnchg0WehO+fucs5lO+dygBuADc657504xszGAM8C33fO7RyQpOK3lPhoHlyaz+7qRh58ZYfXcUTkPDnn+9DNbLmZLfe9vBdIA/5oZiVmVhyQdHLOLpuUwY2XjOVf3i3n77v1DyaRwcC8mgNk1qxZrrhYvT+Qmto6uO5/vUN7p+Pln15Gcly015FEpJ/MbKNzblZv2/SkaBhLiIni4WUzqKxrZtW6rV7HEZEBpkIPczPHDmX51ydQWHyQ17Ye9jqOiAwgFfog8NOrJpObOYQ7ny3jyLFWr+OIyABRoQ8CMVHdc6fXNbdzz/ObNXe6SJhSoQ8SuZlDWHH1Bby0+RAvlFR4HUdEBoAKfRC5ae54Zo4dyi9e2ExlXbPXcUQkwFTog0hkhPHw0gI6Oh0rnynTpReRMKNCH2Ry0hO5e0Eu73xaw1Mf7PM6jogEkAp9EPruxWOYOzmD36zfTnlNo9dxRCRAVOiDkJmx+vp8oiON2wpL6NSydSJhQYU+SI1MiWPVkul8vP8oj7292+s4IhIAKvRBbHFBFgvyMvntqzvZWlHvdRwR6ScV+iBmZqxaMp2U+BhWFJbQ2qG500VCmQp9kBuWGMMD1+ex/VADj772qddxRKQfVOjClbkj+M6s0Tz21m427qv1Oo6InCMVugBwz8JcslLjWVFYSlNbh9dxROQcqNAFgOS4aB5aWsD+2ibuX7/d6zgicg5U6HLc7PFp/GjOOP7PB/t4e2e113FE5Cyp0OUkt19zAROHJ7HymTLqmtq9jiMiZ0GFLieJi47kt8tmUHOslfvWbvY6joicBRW6nCIvO4Wb503i+ZIK1m+q9DqOiPhJhS69+skVE8jPTuHu5zZR1dDidRwR8YMKXXoVHdm9bF1TWyd3/XWT5k4XCQEqdDmticOTWXntFF7fXsWa4oNexxGRPqjQ5Yx+eGkOs8cP4x+LtnCgtsnrOCJyBip0OaOICOOhpQWYGbevKaVLc6eLBC0VuvQpe2gC9y6ayofltTz5XrnXcUTkNFTo4pelM7O5Knc4q1/ZwaeHG7yOIyK9UKGLX8yM+7+VT1JsFCsKS2nv7PI6koj0oEIXv2Ukx/LrJdPZ9Fkdf3hjl9dxRKQHvwvdzCLN7BMzW9fLNjOzfzazXWZWZmYXBTamBIv5eZl888JR/G7DLsoOHvU6joic4GzO0G8Btp1m23xgku/jJuB/9zOXBLFfLp5GRlIsKwpLaWnXsnUiwcKvQjezbGAB8MRphnwD+HfX7QMg1cwyA5RRgkxKfDQPLs1nV9UxHnxlh9dxRMTH3zP0R4GVwOneCRsFHDjh9UHf105iZjeZWbGZFVdXa77tUHbZpAy+P3ssT75Xzt93H/E6jojgR6Gb2UKgyjm38UzDevnaKU+gOOced87Ncs7NysjIOIuYEozuum4KY4clcPuaUhpaNHe6iNf8OUOfAyw2s73A08A8M3uqx5iDwOgTXmcDFQFJKEErISaKh5fNoLKumX9ad7q3V0TkfOmz0J1zdznnsp1zOcANwAbn3Pd6DFsL3Oi722U2UOec00Tag8DMsUNZ/vUJ/KX4AE+8s4d6namLeCbqXP+gmS0HcM79CVgPXAfsApqAHwYknYSEn141mY/Ka/mnF7ex+uUdXH5BBotnZHHllBHEx0R6HU9k0DCv5rmeNWuWKy4u9uRnS+A55yg5cJSi0krWlVVQ1dBKQkwkV+WOYHFBFnMnZxATpefYRPrLzDY652b1uk2FLoHW2eX4qLyWorIKXtpUyedN7QyJi2L+9EwWFWRxyYQ0IiN6ex9dRPqiQhfPtHd28e6uGopKKvjb1sMca+0gPSmWBXkjWVSQxUVjhhKhchfxmwpdgkJLeydvbK+iqKyC17dV0drRxajUeBbmd5+5T8sagpnKXeRMVOgSdI61dvDq1kMUlVby9s5qOroc49MTWViQxeKCLCYOT/I6okhQUqFLUPu8sY2XtxyiqLSCv+85gnOQmzmExQVZLMzPZPSwBK8jigQNFbqEjKr6Fl7cVMna0go+2X8UgAvHpLK4IIsFeZkMHxLnbUARj6nQJSQdqG1iXVl3uW+rrCfCYPb4NBYVZDF/+khSE2K8jihy3qnQJeTtqmpgbWklRaUVlNc0EhVhzJ2cwaKCTK6eOpKk2HN+Rk4kpKjQJWw459hSUU9RaQVFpRVU1LUQGxXBlbnDWZSfxRVThhMXradTJXyp0CUsdXU5Pt7/OUWlFby4qZKaY20kxUbxD1NHsGhGFl+bmE50pJ5OlfCiQpew19HZxQd7aikqreClzZXUt3QwNCGa+XmZLMrP4qvjhunpVAkLKnQZVFo7OnlnZw1rSyt4dethmts7GZ4cy8L8LBYVZDJjdKoeYJKQpUKXQauprYMN26tYW1LBmzuqaevsYvSweBblZ7GoIIspI5NV7hJSVOgiQH1LO3/bcpi1pRW8t6uGzi7HpOFJLCroLvdx6YleRxTpkwpdpIcjx1pZv7n76dSPymsByBuVwqKCTBbmZ5GVGu9xQpHeqdBFzqCyrpkXy7rvcS89WAfAV3KGsrggi/l5maQnxXqcUORLKnQRP+2taWRdWQVrSyvYefgYEQZzJqazqCCLa6aNJCU+2uuIMsip0EXOwY5DDRSVdpf7/tomYiIjmDu5e3m9q3KHkxCjp1Pl/FOhi/SDc46yg3WsLa1gXVkFh+tbiY+O5KqpI1iUn8nXL8ggNkpPp8r5oUIXCZCuLsd/7q1lbWkF633L6yXHRXHttO4VmC6dkEaUnk6VAaRCFxkA7Z1dvLerhqLSSv625RANrR2kJcZwXV4mi2dkMVPL68kAUKGLDLCW9k7e3FHtW17vMC3tXWSmxLEwP5PFBaOYPkrL60lgqNBFzqNjrR28vu0wRaUVvLWzmvZOR05aAot8y+tNGpHsdUQJYSp0EY8cbWrjlS2HWFtawd93H6HLwZSRyd1Pp+ZnMSZNy+vJ2VGhiwSBqoYW1pdVUlRWycZ9nwMwY3Qqi3xrp47Q8nriBxW6SJA5+Hn38npFpRVsqajHDC4eN8y3vF4mwxK1vJ70ToUuEsR2Vx87/gDTnuru5fW+NimdRflZ/MO0ESTH6elU+ZIKXSQEOOfYWllPkW/t1M+ONhMTFcG8C4azeEYW87S8nqBCFwk5zjk+3n/0+PJ61Q2tJMZEcvXUESyekcXXJmYQE6UHmAajfhW6mcUBbwOxQBTwjHPuvh5jUoCngDG+MQ855/71TN9XhS7in84ux4d7jlBUVsH6TYeoa24nNSGa+dNHsig/i4vHp2l5vUGkv4VuQKJz7piZRQPvArc45z44YczPgRTn3B1mlgHsAEY659pO931V6CJnr62ji3d3VbO2pIK/bT1MU1snGcmxLMjLZFFBFheN0fJ64e5Mhd7ndHGuu/GP+V5G+z56/hZwQLKv/JOAWqDjnBOLSK9ioiKYN2UE86aMoLmtkw3bqygqreD/frSfP7+/l1Gp8b4VmDKZmqmnUwcbv66hm1kksBGYCPzBOXdHj+3JwFpgCpAMfMc59+KZvqfO0EUCp8G3vF5RWQXvfNq9vN6EjESunT6SySOSyUlLJCc9UfO5h4GAvSlqZqnAc8DNzrnNJ3z928AcYAUwAXgVKHDO1ff48zcBNwGMGTNm5r59+85uT0SkT7WNbby0uftOmQ/Laznxr/iwxBhy0hLISU9knK/kx6V3/zcpVvO7h4KA3uViZvcBjc65h0742ovA/3TOveN7vQG40zn30em+j87QRQZeS3sn+440UV7TyN4jjeytaTz++eH61pPGpifFMi494fjZ/Lj0RN/nCVrMI4j06xq6703OdufcUTOLB64CHugxbD9wJfCOmY0ALgD29C+2iPRXXHQkF4xM5oKRp04I1tTWwd6aJvYe8ZW8r+jf2FFNzcaDJ40dOSSOnPSEE0q+u/DHDEvQvfFBxJ9fu5nAv/muo0cAhc65dWa2HMA59ydgFfBnM9sEGHCHc65moEKLSP8lxEQxNWsIU7OGnLKtoaX9yzP7mkbKfWf3r2w5TG3jlzevmUFWSjw5vjP7Ewt/zLAE3St/nunBIhE5K3XN7cfP5r8s/Cb21jRS19x+fFyEwaih8ScV/RfX67OHxhOtlZ3OSb8uuYiInCglPpqC0akUjE49ZdvnjW3Hz+ZPLPrnPv6MhtYv72SOijCyh8aT06Pox6UlMmpovB6UOkcqdBEJmKGJMQxNjOGiMUNP+rpzjiONbSe9KVte00h5TRMfldfS1NZ5fGx0pDF6WMLxu3C+KPpxGYlkDonTsn5noEIXkQFnZqQnxZKeFMusnGEnbXPOUdXQesr1+r01Tby7q4bWjq7jY2OjIhiblnDSWf0Xn48YEjvoH6RSoYuIp8yMEUPiGDEkjtnj007a1tXlOFTfclLRl9c0saemkTd3VNPW+WXZx0dHMjYt4aTLN91n+AlkJA2Oslehi0jQiogwslLjyUqN59KJ6Sdt6+xyVBxtPuH++u5bMHccauDVrYfp6Pryho+k2KjuM/uTHqjqPtMflhgTNmWvQheRkBQZ0X2tffSwBC6blHHSto7OLj472nzC/fXdt2Bu/qyOlzcfovOEsk+Oi+pxf/2Xl3RSE0Jr5SgVuoiEnajICMamJTI2LbH7MccTtHV0cfDzLx6oajp+C+bH+z+nqKzipKkSUhOiyUlLZHz6yW/Q5qQnBOVKUip0ERlUYqIiGJ+RxPiMpFO2tXZ0cqC2iT3VjScV/t/3HOHZTz47aWx6Ukyv0yTkpCWS6NG8OCp0ERGf2KhIJg5PZuLwU6dKaG7rZF/tCdfrfW/Uvr2zmmd6TJUwPDn21Ov1vtIfyKkSVOgiIn6Ij4lkysghTBl56lQJja0dvjdnT54b5/Xth6k5dvI6P5kpcfzXOeP473PHBzyjCl1EpJ8SY6OYlpXCtKyUU7bVt7Szr6bppCdohw+JHZAcKnQRkQE0JC6avOwU8rJPLftA0+w4IiJhQoUuIhImVOgiImFChS4iEiZU6CIiYUKFLiISJlToIiJhQoUuIhImPFsk2syqgX3n+MfTgZoAxvGS9iU4hcu+hMt+gPblC2Odcxm9bfCs0PvDzIpPt+p1qNG+BKdw2Zdw2Q/QvvhDl1xERMKECl1EJEyEaqE/7nWAANK+BKdw2Zdw2Q/QvvQpJK+hi4jIqUL1DF1ERHpQoYuIhImgLnQzu9bMdpjZLjO7s5ftZmb/7NteZmYXeZHTH37sy+VmVmdmJb6Pe73I2Rcze9LMqsxs82m2h9Ix6WtfQuWYjDazN8xsm5ltMbNbehkTEsfFz30JleMSZ2YfmVmpb1/+sZcxgT0uzrmg/AAigd3AeCAGKAWm9hhzHfASYMBs4EOvc/djXy4H1nmd1Y99mQtcBGw+zfaQOCZ+7kuoHJNM4CLf58nAzhD+u+LPvoTKcTEgyfd5NPAhMHsgj0swn6F/FdjlnNvjnGsDnga+0WPMN4B/d90+AFLNLPN8B/WDP/sSEpxzbwO1ZxgSKsfEn30JCc65Sufcx77PG4BtwKgew0LiuPi5LyHB9//6mO9ltO+j510oAT0uwVzoo4ADJ7w+yKkH1p8xwcDfnJf4/nn2kplNOz/RAi5Ujom/QuqYmFkOcCHdZ4MnCrnjcoZ9gRA5LmYWaWYlQBXwqnNuQI9LMC8Sbb18redvN3/GBAN/cn5M9xwNx8zsOuB5YNJABxsAoXJM/BFSx8TMkoC/Aj91ztX33NzLHwna49LHvoTMcXHOdQIzzCwVeM7MpjvnTnzPJqDHJZjP0A8Co094nQ1UnMOYYNBnTudc/Rf/PHPOrQeizSz9/EUMmFA5Jn0KpWNiZtF0F+B/OOee7WVIyByXvvYllI7LF5xzR4E3gWt7bArocQnmQv9PYJKZjTOzGOAGYG2PMWuBG33vFM8G6pxzlec7qB/63BczG2lm5vv8q3QfmyPnPWn/hcox6VOoHBNfxn8BtjnnHjnNsJA4Lv7sSwgdlwzfmTlmFg9cBWzvMSygxyVoL7k45zrM7H8Ar9B9l8iTzrktZrbct/1PwHq63yXeBTQBP/Qq75n4uS/fBn5sZh1AM3CD870NHkzM7P/RfZdBupkdBO6j+82ekDom4Ne+hMQxAeYA3wc2+a7XAvwcGAMhd1z82ZdQOS6ZwL+ZWSTdv3QKnXPrBrLD9Oi/iEiYCOZLLiIichZU6CIiYUKFLiISJlToIiJhQoUuIhImVOgiImFChS4iEib+P1CH89zGG3iuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(INPUT_DIM, hidden_size, 4).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, OUTPUT_DIM, 4, dropout_p=0.1).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, epochs = 1, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly evaluate\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for _, (src, trg, tgt_output) in enumerate(test_iter):\n",
    "        for i in range(n):\n",
    "#             print(src.shape)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "#             print(src_[i])\n",
    "#             print(src_[i].unsqueeze(dim=1).shape)\n",
    "\n",
    "            src_ = src_[i]\n",
    "            trg_ = trg_[i]\n",
    "            print('>', german_id_to_text(src_))\n",
    "            print('=', english_id_to_text(trg_))\n",
    "            output_words, attentions = evaluate(encoder, decoder, src_.unsqueeze(dim=1))\n",
    "#             output_sentence = ' '.join(output_words)\n",
    "            print('<', output_words)\n",
    "            print('')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['<bos>', 'Eine', '<unk>', 'mit', 'einer', 'Violine', 'spielt', 'auf', 'der', 'Straße', 'während', 'eine', 'Frau', 'mit', 'einer', 'blauen', 'Gitarre', 'zusieht.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "= ['<bos>', 'A', 'female', 'performer', 'with', 'a', 'violin', 'plays', 'on', 'a', 'street', 'while', 'a', 'woman', 'with', 'a', 'blue', 'guitar', 'looks', 'on.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "< [['A'], ['man'], ['in'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], '<EOS>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation is hard task!!\n",
    "For this work: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "Training duration is quite long, as well as the dataset may not be sufficient for machine translation network.\n",
    "However, in this case it is enough in order to understand the purpose and learn the new techniques.\n",
    "\n",
    "Implement and test Pytorch’s own Transformer models for machine translation\n",
    "Analyze based on the output and measurements.\n",
    "Compare to provided example seq2seq\n",
    "\n",
    "# Lab Assignment Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import (TransformerEncoder, TransformerDecoder,\n",
    "                      TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "                 emb_size: int, src_vocab_size: int, tgt_vocab_size: int,\n",
    "                 dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=emb_size, nhead=NHEAD,\n",
    "                                                dim_feedforward=dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor,\n",
    "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None,\n",
    "                                        tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding +\n",
    "                            self.pos_embedding[:token_embedding.size(0),:])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of our model and instantiate it.\n",
    "# Define loss function and optimizer - Train model further down.\n",
    "SRC_VOCAB_SIZE = len(de_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 3 #not too many epochs\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"cpu\")\n",
    "#print(\"device\", DEVICE)\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                                 EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE,\n",
    "                                 FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, optimizer,print_every=50, plot_every=100,):\n",
    "    start_time = time.time()\n",
    "    plot_losses = []\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, sample in enumerate(train_iter):\n",
    "        #print(\"s\", len(sample))\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "        src = sample[0]\n",
    "        tgt = sample[1]\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if idx % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / BATCH_SIZE\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start_time, (idx+1) / len(train_iter)),\n",
    "                                        idx, (idx+1)/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "        if idx % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / BATCH_SIZE\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    end_time = time.time()\n",
    "    return losses / len(train_iter), plot_losses\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, sample in (enumerate(valid_iter)):\n",
    "       \n",
    "        src = sample[0]\n",
    "        tgt = sample[1]\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,\n",
    "                                  src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    #print(\"s1\", len(sample))\n",
    "    return losses / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "EPOCH:  1  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0758\n",
      "0m 14s (50 22%) 0.0512\n",
      "0m 29s (100 44%) 0.0435\n",
      "0m 44s (150 66%) 0.0402\n",
      "0m 58s (200 88%) 0.0385\n",
      "Epoch: 1, Train loss: 5.871, Val loss: 4.551\n",
      "-----------------------\n",
      "EPOCH:  2  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0366\n",
      "0m 14s (50 22%) 0.0346\n",
      "0m 29s (100 44%) 0.0333\n",
      "0m 43s (150 66%) 0.0313\n",
      "0m 58s (200 88%) 0.0308\n",
      "Epoch: 2, Train loss: 4.194, Val loss: 3.752\n",
      "-----------------------\n",
      "EPOCH:  3  out of  3  epochs\n",
      "-----------------------\n",
      "0m 0s (0 0%) 0.0302\n",
      "0m 14s (50 22%) 0.0288\n",
      "0m 28s (100 44%) 0.0265\n",
      "0m 42s (150 66%) 0.0274\n",
      "0m 58s (200 88%) 0.0261\n",
      "Epoch: 3, Train loss: 3.571, Val loss: 3.354\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    print(\"-----------------------\")\n",
    "    print(\"EPOCH: \", epoch, \" out of \", NUM_EPOCHS, \" epochs\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    train_loss, plot_losses = train(transformer, train_iter, optimizer)\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))\n",
    "\n",
    "#showPlot(plot_losses) #could not run this, got an error which never occured previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(device)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(device).type(torch.bool)\n",
    "        tgt_mask = (generate_mask(ys.size(0))\n",
    "                                    .type(torch.bool)).to(device)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
    "    model.eval()\n",
    "    tokens = [BOS_IDX] + [src_vocab.stoi[tok] for tok in src_tokenizer(src)]+ [EOS_IDX]\n",
    "    num_tokens = len(tokens)\n",
    "    src = (torch.LongTensor(tokens).reshape(num_tokens, 1) )\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join([tgt_vocab.itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A group of people standing in front of a large building. '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", de_vocab, en_vocab, de_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Below\n",
    "For this work: https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "### Generating names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read()\n",
    "    lines = lines.strip()\n",
    "    lines = lines.split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('C:/Users/jason/Python/Jason/Done/2-language-models-lab/data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same functions as those in 2-text-classifier.\n",
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "lr = 0.005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-lr)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 23s (10000 12%) 1.4949\n",
      "0m 46s (20000 25%) 2.0176\n",
      "1m 9s (30000 37%) 2.2018\n",
      "1m 31s (40000 50%) 2.4917\n",
      "1m 54s (50000 62%) 1.2929\n",
      "2m 17s (60000 75%) 2.4570\n",
      "2m 39s (70000 87%) 1.8723\n",
      "3m 2s (80000 100%) 1.7989\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 256, n_letters)\n",
    "#rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "n_iters = 80000\n",
    "print_every = 10000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot every iter\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    output, loss = train(*randomTrainingExample())\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b3b12b1a30>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Loss change during training')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------ENG---------\n",
      "Allert\n",
      "Jandel\n",
      "Mottell\n",
      "---------RUS---------\n",
      "Roveky\n",
      "Ungolov\n",
      "Shithivek\n",
      "---------KOR---------\n",
      "Kou\n",
      "Law\n",
      "Iwo\n",
      "---------GRE---------\n",
      "Gorsanos\n",
      "Tskitope\n",
      "Stamates\n",
      "---------CHI---------\n",
      "Chi\n",
      "Lang\n",
      "Hangg\n"
     ]
    }
   ],
   "source": [
    "name_length = 20\n",
    "\n",
    "# Sample from a country/language and starting letter\n",
    "def sample(language, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(language)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(name_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get several samples from one country/language\n",
    "# and starting letters of each name\n",
    "def samples(language, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(language, start_letter))\n",
    "        \n",
    "print(\"---------ENG---------\")\n",
    "samples('English','AJM')\n",
    "print(\"---------RUS---------\")\n",
    "samples('Russian', 'RUS')\n",
    "print(\"---------KOR---------\")\n",
    "samples('Korean', 'KLI')\n",
    "print(\"---------GRE---------\")\n",
    "samples('Greek', 'GTS')\n",
    "print(\"---------CHI---------\")\n",
    "samples('Chinese', 'CLH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMLdl] *",
   "language": "python",
   "name": "conda-env-AMLdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
